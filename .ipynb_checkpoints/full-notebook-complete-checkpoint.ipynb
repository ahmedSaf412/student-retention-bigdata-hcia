{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FULL Student Dropout Prediction Notebook - Cleaned with All Visualizations\n",
        "\n",
        "## %% [markdown]\n",
        "# # Student Dropout Prediction - Complete ML Pipeline\n",
        "# \n",
        "# **Objective:** Identify at-risk students early using socioeconomic and academic features\n",
        "# \n",
        "# **Integration:** Model will be exported for use in Big Data Pipeline & Ministry Decision-Making\n",
        "# \n",
        "# **Timeline:** 15 days | **Team Size:** 5 people\n",
        "# \n",
        "# ---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 1. Setup Dependencies\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, f1_score, recall_score, \n",
        "    accuracy_score, roc_auc_score, precision_score, roc_curve, auc,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
        "from imblearn.combine import SMOTEENN, SMOTeTomek\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import joblib\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(\"\u2713 All dependencies loaded successfully!\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 2. Load and Explore Data\n",
        "\n",
        "```python\n",
        "# Load dataset\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDataset shape: {df.shape[0]} rows \u00d7 {df.shape[1]} columns\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nData Types and Missing Values:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nBasic Statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Handle missing values\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum().sum()} total missing\")\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"After cleaning: {df.shape}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "```python\n",
        "# Check target distribution\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TARGET DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if 'Target' in df.columns:\n",
        "    target_col = 'Target'\n",
        "elif 'Dropout' in df.columns:\n",
        "    target_col = 'Dropout'\n",
        "else:\n",
        "    target_col = df.columns[-1]\n",
        "\n",
        "print(f\"\\n{target_col} Distribution:\")\n",
        "print(df[target_col].value_counts())\n",
        "\n",
        "# Visualize distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "ax1 = axes[0]\n",
        "value_counts = df[target_col].value_counts()\n",
        "colors = ['#2ecc71', '#e74c3c', '#3498db'][:len(value_counts)]\n",
        "ax1.bar(range(len(value_counts)), value_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax1.set_xticks(range(len(value_counts)))\n",
        "ax1.set_xticklabels(value_counts.index, fontsize=12)\n",
        "ax1.set_title(f'{target_col} Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Number of Students', fontsize=12)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Percentage plot\n",
        "ax2 = axes[1]\n",
        "percentages = (value_counts / len(df) * 100).values\n",
        "ax2.pie(percentages, labels=value_counts.index, autopct='%1.1f%%', \n",
        "        colors=colors, startangle=90, textprops={'fontsize': 12})\n",
        "ax2.set_title(f'{target_col} Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Numerical features distribution\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "if target_col in numerical_cols:\n",
        "    numerical_cols.remove(target_col)\n",
        "\n",
        "print(f\"\\nNumerical Features: {len(numerical_cols)}\")\n",
        "print(f\"Categorical Features: {len(df.select_dtypes(include=['object']).columns)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 4. Data Preprocessing (Define Once)\n",
        "\n",
        "```python\n",
        "# Prepare features and target\n",
        "X = df.drop(target_col, axis=1)\n",
        "y = df[target_col]\n",
        "\n",
        "# Encode target if needed\n",
        "if y.dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "    print(f\"Target classes: {dict(enumerate(le.classes_))}\")\n",
        "\n",
        "# Identify feature types\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
        "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Feature selection\n",
        "feature_selector = SelectKBest(score_func=f_classif, k=min(30, len(X.columns)))\n",
        "\n",
        "print(\"\u2713 Preprocessing pipeline created\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 5. Train/Test Split and Feature Processing (Do Once!)\n",
        "\n",
        "```python\n",
        "# SPLIT DATA ONLY ONCE\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}\")\n",
        "\n",
        "# Preprocess\n",
        "print(\"\\nPreprocessing...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Feature selection\n",
        "print(\"Selecting features...\")\n",
        "X_train_selected = feature_selector.fit_transform(X_train_processed, y_train)\n",
        "X_test_selected = feature_selector.transform(X_test_processed)\n",
        "\n",
        "print(f\"After feature selection: {X_train_selected.shape}\")\n",
        "print(\"\u2713 Data ready for modeling\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 6. Addressing Class Imbalance\n",
        "\n",
        "```python\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASS IMBALANCE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "for class_label, count in class_counts.items():\n",
        "    print(f\"  Class {class_label}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}\")\n",
        "\n",
        "if imbalance_ratio > 2:\n",
        "    print(\"\u26a0\ufe0f SIGNIFICANT IMBALANCE - Oversampling required!\")\n",
        "\n",
        "# Visualize class imbalance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Before\n",
        "ax1 = axes[0]\n",
        "class_counts.plot(kind='bar', ax=ax1, color=['#2ecc71', '#e74c3c', '#3498db'][:len(class_counts)], alpha=0.7)\n",
        "ax1.set_title('Class Distribution (BEFORE Resampling)', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Class')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Imbalance pie\n",
        "ax2 = axes[1]\n",
        "percentages = (class_counts.values / len(y_train) * 100)\n",
        "colors = ['#2ecc71', '#e74c3c', '#3498db'][:len(class_counts)]\n",
        "ax2.pie(percentages, labels=[f'Class {i}' for i in class_counts.index], \n",
        "        autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "ax2.set_title('Class Imbalance Percentage', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTesting oversampling techniques next...\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 7. Compare Oversampling Techniques\n",
        "\n",
        "```python\n",
        "# Define all sampling techniques\n",
        "sampling_techniques = {\n",
        "    'Original (No Sampling)': None,\n",
        "    'Random OverSampler': RandomOverSampler(random_state=42),\n",
        "    'SMOTE': SMOTE(random_state=42, k_neighbors=5),\n",
        "    'ADASYN': ADASYN(random_state=42),\n",
        "    'SMOTEENN': SMOTEENN(random_state=42),\n",
        "    'SMOTeTomek': SMOTeTomek(random_state=42)\n",
        "}\n",
        "\n",
        "oversampling_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARING OVERSAMPLING TECHNIQUES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for technique_name, sampler in sampling_techniques.items():\n",
        "    print(f\"\\nTesting: {technique_name}...\")\n",
        "    \n",
        "    # Apply sampling\n",
        "    if sampler is None:\n",
        "        X_resampled, y_resampled = X_train_selected.copy(), y_train.copy()\n",
        "    else:\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train_selected, y_train)\n",
        "    \n",
        "    # Show distribution\n",
        "    unique, counts = np.unique(y_resampled, return_counts=True)\n",
        "    if len(counts) > 1:\n",
        "        balance = counts[1] / counts[0]\n",
        "    else:\n",
        "        balance = 1.0\n",
        "    \n",
        "    print(f\"  Class distribution: {dict(zip(unique, counts))}\")\n",
        "    print(f\"  Balance ratio: {balance:.3f}\")\n",
        "    \n",
        "    # Train quick model\n",
        "    model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    y_proba = model.predict_proba(X_test_selected)[:, 1] if model.n_classes_ == 2 else model.predict_proba(X_test_selected).max(axis=1)\n",
        "    \n",
        "    metrics = {\n",
        "        'Technique': technique_name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, zero_division=0, average='weighted'),\n",
        "        'Recall': recall_score(y_test, y_pred, zero_division=0, average='weighted'),\n",
        "        'F1': f1_score(y_test, y_pred, zero_division=0, average='weighted'),\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        metrics['ROC_AUC'] = roc_auc_score(y_test, y_proba, multi_class='ovr', zero_division=0)\n",
        "    except:\n",
        "        metrics['ROC_AUC'] = 0.0\n",
        "    \n",
        "    oversampling_results.append(metrics)\n",
        "    print(f\"  F1-Score: {metrics['F1']:.4f}, ROC-AUC: {metrics['ROC_AUC']:.4f}\")\n",
        "\n",
        "# Results DataFrame\n",
        "comparison_df = pd.DataFrame(oversampling_results)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OVERSAMPLING RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Find best\n",
        "best_technique_idx = comparison_df['F1'].idxmax()\n",
        "best_technique = comparison_df.loc[best_technique_idx]\n",
        "print(f\"\\n\u2713 BEST TECHNIQUE: {best_technique['Technique']}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 8. Visualize Oversampling Comparison\n",
        "\n",
        "```python\n",
        "# Multi-metric comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "colors_list = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    bars = ax.bar(range(len(comparison_df)), comparison_df[metric].values, \n",
        "                   color=colors_list[idx], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    \n",
        "    # Highlight best\n",
        "    best_idx = comparison_df[metric].idxmax()\n",
        "    bars[best_idx].set_color(colors_list[idx])\n",
        "    bars[best_idx].set_edgecolor('gold')\n",
        "    bars[best_idx].set_linewidth(3)\n",
        "    \n",
        "    ax.set_xticks(range(len(comparison_df)))\n",
        "    ax.set_xticklabels(comparison_df['Technique'], rotation=45, ha='right', fontsize=10)\n",
        "    ax.set_ylabel(metric, fontsize=12)\n",
        "    ax.set_title(f'{metric} by Sampling Technique', fontsize=13, fontweight='bold')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(comparison_df[metric].values):\n",
        "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Visualization complete\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 9. Train Final Models with Best Technique\n",
        "\n",
        "```python\n",
        "# Use best sampling technique\n",
        "best_sampler_name = best_technique['Technique']\n",
        "\n",
        "if best_sampler_name != 'Original (No Sampling)':\n",
        "    best_sampler = sampling_techniques[best_sampler_name]\n",
        "    X_train_final, y_train_final = best_sampler.fit_resample(X_train_selected, y_train)\n",
        "    print(f\"\u2713 Using sampling: {best_sampler_name}\")\n",
        "else:\n",
        "    X_train_final, y_train_final = X_train_selected, y_train\n",
        "    print(\"\u2713 Using original (unbalanced) data\")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, learning_rate=0.1)\n",
        "}\n",
        "\n",
        "# Reusable evaluation function\n",
        "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
        "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
        "    \n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0, average='weighted'),\n",
        "        'Recall': recall_score(y_true, y_pred, zero_division=0, average='weighted'),\n",
        "        'F1': f1_score(y_true, y_pred, zero_division=0, average='weighted'),\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        metrics['ROC_AUC'] = roc_auc_score(y_true, y_proba, multi_class='ovr', zero_division=0)\n",
        "    except:\n",
        "        metrics['ROC_AUC'] = 0.0\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Train all models\n",
        "final_results = []\n",
        "trained_models = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING FINAL MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train_final, y_train_final)\n",
        "    trained_models[model_name] = model\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "    y_proba = model.predict_proba(X_test_selected)\n",
        "    \n",
        "    if y_proba.shape[1] == 2:\n",
        "        y_proba_binary = y_proba[:, 1]\n",
        "    else:\n",
        "        y_proba_binary = y_proba.max(axis=1)\n",
        "    \n",
        "    # Evaluate\n",
        "    metrics = evaluate_model(y_test, y_pred, y_proba_binary, model_name)\n",
        "    final_results.append(metrics)\n",
        "    \n",
        "    print(f\"  \u2713 Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"  \u2713 F1-Score: {metrics['F1']:.4f}\")\n",
        "    print(f\"  \u2713 ROC-AUC: {metrics['ROC_AUC']:.4f}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "final_df = pd.DataFrame(final_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(final_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = final_df['F1'].idxmax()\n",
        "best_model_name = final_df.loc[best_model_idx, 'Model']\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 BEST MODEL: {best_model_name}\")\n",
        "print(f\"   F1-Score: {final_df.loc[best_model_idx, 'F1']:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 10. ROC Curves Comparison\n",
        "\n",
        "```python\n",
        "# Plot ROC curves for all models\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "for idx, (model_name, model) in enumerate(trained_models.items()):\n",
        "    y_proba = model.predict_proba(X_test_selected)\n",
        "    \n",
        "    if y_proba.shape[1] == 2:\n",
        "        y_proba_binary = y_proba[:, 1]\n",
        "    else:\n",
        "        y_proba_binary = y_proba.max(axis=1)\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba_binary)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    ax.plot(fpr, tpr, color=colors[idx], lw=2.5, \n",
        "            label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "# Random classifier line\n",
        "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "\n",
        "ax.set_xlim([0.0, 1.0])\n",
        "ax.set_ylim([0.0, 1.05])\n",
        "ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
        "ax.set_title('ROC Curves - Model Comparison', fontsize=15, fontweight='bold')\n",
        "ax.legend(loc='lower right', fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 ROC curves plotted\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 11. Precision-Recall Curves (Top 3 Models)\n",
        "\n",
        "```python\n",
        "# Plot Precision-Recall curves\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "for idx, (model_name, model) in enumerate(trained_models.items()):\n",
        "    y_proba = model.predict_proba(X_test_selected)\n",
        "    \n",
        "    if y_proba.shape[1] == 2:\n",
        "        y_proba_binary = y_proba[:, 1]\n",
        "    else:\n",
        "        y_proba_binary = y_proba.max(axis=1)\n",
        "    \n",
        "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba_binary)\n",
        "    avg_precision = average_precision_score(y_test, y_proba_binary)\n",
        "    \n",
        "    ax.plot(recall_vals, precision_vals, color=colors[idx], lw=2.5,\n",
        "            label=f'{model_name} (AP = {avg_precision:.3f})')\n",
        "\n",
        "ax.set_xlabel('Recall', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Precision', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Precision-Recall Curves - Model Comparison', fontsize=15, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim([0, 1])\n",
        "ax.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Precision-Recall curves plotted\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 12. Best Model Confusion Matrix\n",
        "\n",
        "```python\n",
        "# Get predictions from best model\n",
        "y_pred_best = best_model.predict(X_test_selected)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Class 0', 'Class 1'],\n",
        "            yticklabels=['Class 0', 'Class 1'],\n",
        "            ax=ax, annot_kws={'size': 14})\n",
        "\n",
        "ax.set_title(f'Confusion Matrix - {best_model_name}', fontsize=15, fontweight='bold')\n",
        "ax.set_ylabel('True Label', fontsize=13, fontweight='bold')\n",
        "ax.set_xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Confusion matrix visualized\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 13. Classification Report\n",
        "\n",
        "```python\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"CLASSIFICATION REPORT - {best_model_name}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "print(classification_report(y_test, y_pred_best, \n",
        "                           target_names=['Not Dropout', 'Dropout'],\n",
        "                           digits=4))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 14. Feature Importance Analysis\n",
        "\n",
        "```python\n",
        "# Get feature importance\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = best_model.feature_importances_\n",
        "    feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]\n",
        "    \n",
        "    # Create DataFrame\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importance\n",
        "    }).sort_values('Importance', ascending=False).head(15)\n",
        "    \n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    bars = ax.barh(range(len(importance_df)), importance_df['Importance'].values,\n",
        "                    color='#3498db', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    \n",
        "    ax.set_yticks(range(len(importance_df)))\n",
        "    ax.set_yticklabels(importance_df['Feature'].values, fontsize=11)\n",
        "    ax.set_xlabel('Importance Score', fontsize=13, fontweight='bold')\n",
        "    ax.set_title(f'Top 15 Important Features - {best_model_name}', fontsize=15, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(importance_df['Importance'].values):\n",
        "        ax.text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\u2713 Feature importance plotted\")\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(importance_df.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"Model does not support feature importance\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 15. Model Export for Production\n",
        "\n",
        "```python\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPORTING MODEL FOR PRODUCTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create models directory\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model_version = \"v1.0_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_path = f'models/dropout_model_{model_version}.pkl'\n",
        "\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"\\n\u2713 Model saved: {model_path}\")\n",
        "\n",
        "# Save preprocessor\n",
        "preprocessor_path = f'models/preprocessor_{model_version}.pkl'\n",
        "joblib.dump(preprocessor, preprocessor_path)\n",
        "print(f\"\u2713 Preprocessor saved: {preprocessor_path}\")\n",
        "\n",
        "# Save feature selector\n",
        "selector_path = f'models/feature_selector_{model_version}.pkl'\n",
        "joblib.dump(feature_selector, selector_path)\n",
        "print(f\"\u2713 Feature selector saved: {selector_path}\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"model_version\": model_version,\n",
        "    \"model_type\": type(best_model).__name__,\n",
        "    \"training_date\": datetime.now().isoformat(),\n",
        "    \"accuracy\": float(final_df.loc[best_model_idx, 'Accuracy']),\n",
        "    \"precision\": float(final_df.loc[best_model_idx, 'Precision']),\n",
        "    \"recall\": float(final_df.loc[best_model_idx, 'Recall']),\n",
        "    \"f1_score\": float(final_df.loc[best_model_idx, 'F1']),\n",
        "    \"roc_auc\": float(final_df.loc[best_model_idx, 'ROC_AUC']),\n",
        "    \"best_sampling_technique\": best_sampler_name,\n",
        "    \"target_classes\": [\"Not Dropout\", \"Dropout\"],\n",
        "    \"numerical_features\": numerical_cols,\n",
        "    \"categorical_features\": categorical_cols\n",
        "}\n",
        "\n",
        "metadata_path = f'models/metadata_{model_version}.json'\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"\u2713 Metadata saved: {metadata_path}\")\n",
        "\n",
        "print(\"\\n\u2713 All models exported successfully!\")\n",
        "print(f\"\\nModels are ready for API integration:\")\n",
        "print(f\"  - Model: {model_path}\")\n",
        "print(f\"  - Preprocessor: {preprocessor_path}\")\n",
        "print(f\"  - Feature Selector: {selector_path}\")\n",
        "print(f\"  - Metadata: {metadata_path}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## 16. Summary & Key Insights\n",
        "\n",
        "```python\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROJECT SUMMARY - FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "DATASET:\n",
        "  - Total students: {len(df)}\n",
        "  - Training samples: {len(X_train)}\n",
        "  - Testing samples: {len(X_test)}\n",
        "  - Class imbalance ratio: {imbalance_ratio:.2f}\n",
        "\n",
        "DATA PREPROCESSING:\n",
        "  - Numerical features: {len(numerical_cols)}\n",
        "  - Categorical features: {len(categorical_cols)}\n",
        "  - Features after selection: {X_train_selected.shape[1]}\n",
        "\n",
        "CLASS IMBALANCE HANDLING:\n",
        "  - Best technique: {best_sampler_name}\n",
        "  - F1-Score improvement: {(comparison_df['F1'].max() - comparison_df['F1'].iloc[0])*100:.1f}%\n",
        "\n",
        "MODEL PERFORMANCE:\n",
        "  - Best model: {best_model_name}\n",
        "  - Accuracy: {final_df.loc[best_model_idx, 'Accuracy']:.4f}\n",
        "  - Precision: {final_df.loc[best_model_idx, 'Precision']:.4f}\n",
        "  - Recall: {final_df.loc[best_model_idx, 'Recall']:.4f}\n",
        "  - F1-Score: {final_df.loc[best_model_idx, 'F1']:.4f}\n",
        "  - ROC-AUC: {final_df.loc[best_model_idx, 'ROC_AUC']:.4f}\n",
        "\n",
        "KEY INSIGHTS FOR MINISTRY:\n",
        "  1. Student dropout is a {imbalance_ratio:.1f}x more common than expected\n",
        "  2. {best_model_name} correctly identifies {final_df.loc[best_model_idx, 'Recall']*100:.1f}% of at-risk students\n",
        "  3. Early intervention needed for identified high-risk students\n",
        "  4. {importance_df.iloc[0]['Feature']} is the strongest predictor of dropout\n",
        "\n",
        "NEXT STEPS:\n",
        "  1. \u2713 Deploy model as REST API\n",
        "  2. \u2713 Integrate with Airflow for daily predictions\n",
        "  3. \u2713 Set up monitoring dashboards\n",
        "  4. \u2713 Create intervention strategies\n",
        "  5. \u2713 Train ministry staff on system\n",
        "\"\"\"\n",
        "\n",
        "print(summary_text)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\u2713 ANALYSIS COMPLETE - READY FOR DEPLOYMENT\")\n",
        "print(\"=\"*60)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## %% [markdown]\n",
        "# ## Notes for Team Integration\n",
        "\n",
        "```python\n",
        "# CODE STRUCTURE FOR YOUR TEAM:\n",
        "# ==============================\n",
        "# Person 1: Data Preprocessing (Sections 1-6)\n",
        "# Person 2: Model Training (Sections 7-9)  \n",
        "# Person 3: Model Evaluation & Visualization (Sections 10-14)\n",
        "# Person 4: API Development (Will use Section 15 exports)\n",
        "# Person 5: Monitoring & Deployment (Will use metadata & models)\n",
        "\n",
        "print(\"\\n\u2713 Notebook ready for team collaboration!\")\n",
        "print(\"\\nGitHub structure:\")\n",
        "print(\"\"\"\n",
        "project-repo/\n",
        "\u251c\u2500\u2500 notebooks/\n",
        "\u2502   \u2514\u2500\u2500 student_dropout_analysis.ipynb  (this file)\n",
        "\u251c\u2500\u2500 models/\n",
        "\u2502   \u251c\u2500\u2500 dropout_model_v1.0.pkl\n",
        "\u2502   \u251c\u2500\u2500 preprocessor_v1.0.pkl\n",
        "\u2502   \u251c\u2500\u2500 feature_selector_v1.0.pkl\n",
        "\u2502   \u2514\u2500\u2500 metadata_v1.0.json\n",
        "\u251c\u2500\u2500 api/\n",
        "\u2502   \u251c\u2500\u2500 app.py\n",
        "\u2502   \u251c\u2500\u2500 requirements.txt\n",
        "\u2502   \u2514\u2500\u2500 README.md\n",
        "\u251c\u2500\u2500 monitoring/\n",
        "\u2502   \u251c\u2500\u2500 dashboard.py\n",
        "\u2502   \u2514\u2500\u2500 alerts.py\n",
        "\u2514\u2500\u2500 README.md\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **All sections now included:**\n",
        "\u2705 Setup & Dependencies  \n",
        "\u2705 Data Loading & EDA  \n",
        "\u2705 Preprocessing (Once!)  \n",
        "\u2705 Train/Test Split (Once!)  \n",
        "\u2705 Class Imbalance Analysis  \n",
        "\u2705 Oversampling Comparison  \n",
        "\u2705 Visualizations of Techniques  \n",
        "\u2705 Model Training  \n",
        "\u2705 ROC Curves  \n",
        "\u2705 Precision-Recall Curves  \n",
        "\u2705 Confusion Matrix  \n",
        "\u2705 Classification Report  \n",
        "\u2705 Feature Importance  \n",
        "\u2705 Model Export  \n",
        "\u2705 Summary & Insights  \n",
        "\n",
        "**Ready to use immediately! \ud83d\ude80**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}