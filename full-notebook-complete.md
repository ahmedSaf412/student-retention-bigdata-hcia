# FULL Student Dropout Prediction Notebook - Cleaned with All Visualizations

## %% [markdown]
# # Student Dropout Prediction - Complete ML Pipeline
# 
# **Objective:** Identify at-risk students early using socioeconomic and academic features
# 
# **Integration:** Model will be exported for use in Big Data Pipeline & Ministry Decision-Making
# 
# **Timeline:** 15 days | **Team Size:** 5 people
# 
# ---

## %% [markdown]
# ## 1. Setup Dependencies

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, confusion_matrix, f1_score, recall_score, 
    accuracy_score, roc_auc_score, precision_score, roc_curve, auc,
    precision_recall_curve, average_precision_score
)
from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN
from imblearn.combine import SMOTEENN, SMOTeTomek
from imblearn.metrics import geometric_mean_score
import warnings
warnings.filterwarnings('ignore')
import joblib
import json
from datetime import datetime

# Configure plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)

print("‚úì All dependencies loaded successfully!")
```

---

## %% [markdown]
# ## 2. Load and Explore Data

```python
# Load dataset
df = pd.read_csv('your_dataset.csv')

print("="*60)
print("DATASET OVERVIEW")
print("="*60)
print(f"\nDataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns")
print("\nFirst 5 rows:")
display(df.head())

print("\nData Types and Missing Values:")
print(df.info())

print("\nBasic Statistics:")
display(df.describe())

# Handle missing values
print(f"\nMissing values:\n{df.isnull().sum().sum()} total missing")
df = df.dropna()

print(f"After cleaning: {df.shape}")
```

---

## %% [markdown]
# ## 3. Exploratory Data Analysis (EDA)

```python
# Check target distribution
print("\n" + "="*60)
print("TARGET DISTRIBUTION")
print("="*60)

if 'Target' in df.columns:
    target_col = 'Target'
elif 'Dropout' in df.columns:
    target_col = 'Dropout'
else:
    target_col = df.columns[-1]

print(f"\n{target_col} Distribution:")
print(df[target_col].value_counts())

# Visualize distributions
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Count plot
ax1 = axes[0]
value_counts = df[target_col].value_counts()
colors = ['#2ecc71', '#e74c3c', '#3498db'][:len(value_counts)]
ax1.bar(range(len(value_counts)), value_counts.values, color=colors, alpha=0.7, edgecolor='black')
ax1.set_xticks(range(len(value_counts)))
ax1.set_xticklabels(value_counts.index, fontsize=12)
ax1.set_title(f'{target_col} Distribution (Count)', fontsize=14, fontweight='bold')
ax1.set_ylabel('Number of Students', fontsize=12)
ax1.grid(axis='y', alpha=0.3)

# Percentage plot
ax2 = axes[1]
percentages = (value_counts / len(df) * 100).values
ax2.pie(percentages, labels=value_counts.index, autopct='%1.1f%%', 
        colors=colors, startangle=90, textprops={'fontsize': 12})
ax2.set_title(f'{target_col} Distribution (Percentage)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# Numerical features distribution
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
if target_col in numerical_cols:
    numerical_cols.remove(target_col)

print(f"\nNumerical Features: {len(numerical_cols)}")
print(f"Categorical Features: {len(df.select_dtypes(include=['object']).columns)}")
```

---

## %% [markdown]
# ## 4. Data Preprocessing (Define Once)

```python
# Prepare features and target
X = df.drop(target_col, axis=1)
y = df[target_col]

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)
    print(f"Target classes: {dict(enumerate(le.classes_))}")

# Identify feature types
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()

print(f"Numerical columns: {len(numerical_cols)}")
print(f"Categorical columns: {len(categorical_cols)}")

# Create preprocessing pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='passthrough'
)

# Feature selection
feature_selector = SelectKBest(score_func=f_classif, k=min(30, len(X.columns)))

print("‚úì Preprocessing pipeline created")
```

---

## %% [markdown]
# ## 5. Train/Test Split and Feature Processing (Do Once!)

```python
# SPLIT DATA ONLY ONCE
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape}")
print(f"Testing set: {X_test.shape}")

# Preprocess
print("\nPreprocessing...")
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Feature selection
print("Selecting features...")
X_train_selected = feature_selector.fit_transform(X_train_processed, y_train)
X_test_selected = feature_selector.transform(X_test_processed)

print(f"After feature selection: {X_train_selected.shape}")
print("‚úì Data ready for modeling")
```

---

## %% [markdown]
# ## 6. Addressing Class Imbalance

```python
print("\n" + "="*60)
print("CLASS IMBALANCE ANALYSIS")
print("="*60)

class_counts = pd.Series(y_train).value_counts().sort_index()
imbalance_ratio = class_counts.max() / class_counts.min()

print(f"\nClass distribution in training set:")
for class_label, count in class_counts.items():
    print(f"  Class {class_label}: {count} ({count/len(y_train)*100:.1f}%)")

print(f"\nImbalance ratio: {imbalance_ratio:.2f}")

if imbalance_ratio > 2:
    print("‚ö†Ô∏è SIGNIFICANT IMBALANCE - Oversampling required!")

# Visualize class imbalance
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Before
ax1 = axes[0]
class_counts.plot(kind='bar', ax=ax1, color=['#2ecc71', '#e74c3c', '#3498db'][:len(class_counts)], alpha=0.7)
ax1.set_title('Class Distribution (BEFORE Resampling)', fontsize=14, fontweight='bold')
ax1.set_xlabel('Class')
ax1.set_ylabel('Count')
ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)
ax1.grid(axis='y', alpha=0.3)

# Imbalance pie
ax2 = axes[1]
percentages = (class_counts.values / len(y_train) * 100)
colors = ['#2ecc71', '#e74c3c', '#3498db'][:len(class_counts)]
ax2.pie(percentages, labels=[f'Class {i}' for i in class_counts.index], 
        autopct='%1.1f%%', colors=colors, startangle=90)
ax2.set_title('Class Imbalance Percentage', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nTesting oversampling techniques next...")
```

---

## %% [markdown]
# ## 7. Compare Oversampling Techniques

```python
# Define all sampling techniques
sampling_techniques = {
    'Original (No Sampling)': None,
    'Random OverSampler': RandomOverSampler(random_state=42),
    'SMOTE': SMOTE(random_state=42, k_neighbors=5),
    'ADASYN': ADASYN(random_state=42),
    'SMOTEENN': SMOTEENN(random_state=42),
    'SMOTeTomek': SMOTeTomek(random_state=42)
}

oversampling_results = []

print("\n" + "="*60)
print("COMPARING OVERSAMPLING TECHNIQUES")
print("="*60)

for technique_name, sampler in sampling_techniques.items():
    print(f"\nTesting: {technique_name}...")
    
    # Apply sampling
    if sampler is None:
        X_resampled, y_resampled = X_train_selected.copy(), y_train.copy()
    else:
        X_resampled, y_resampled = sampler.fit_resample(X_train_selected, y_train)
    
    # Show distribution
    unique, counts = np.unique(y_resampled, return_counts=True)
    if len(counts) > 1:
        balance = counts[1] / counts[0]
    else:
        balance = 1.0
    
    print(f"  Class distribution: {dict(zip(unique, counts))}")
    print(f"  Balance ratio: {balance:.3f}")
    
    # Train quick model
    model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
    model.fit(X_resampled, y_resampled)
    
    # Evaluate
    y_pred = model.predict(X_test_selected)
    y_proba = model.predict_proba(X_test_selected)[:, 1] if model.n_classes_ == 2 else model.predict_proba(X_test_selected).max(axis=1)
    
    metrics = {
        'Technique': technique_name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, zero_division=0, average='weighted'),
        'Recall': recall_score(y_test, y_pred, zero_division=0, average='weighted'),
        'F1': f1_score(y_test, y_pred, zero_division=0, average='weighted'),
    }
    
    try:
        metrics['ROC_AUC'] = roc_auc_score(y_test, y_proba, multi_class='ovr', zero_division=0)
    except:
        metrics['ROC_AUC'] = 0.0
    
    oversampling_results.append(metrics)
    print(f"  F1-Score: {metrics['F1']:.4f}, ROC-AUC: {metrics['ROC_AUC']:.4f}")

# Results DataFrame
comparison_df = pd.DataFrame(oversampling_results)
print("\n" + "="*60)
print("OVERSAMPLING RESULTS SUMMARY")
print("="*60)
print(comparison_df.to_string(index=False))

# Find best
best_technique_idx = comparison_df['F1'].idxmax()
best_technique = comparison_df.loc[best_technique_idx]
print(f"\n‚úì BEST TECHNIQUE: {best_technique['Technique']}")
```

---

## %% [markdown]
# ## 8. Visualize Oversampling Comparison

```python
# Multi-metric comparison
fig, axes = plt.subplots(2, 2, figsize=(16, 10))
axes = axes.ravel()

metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1']
colors_list = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']

for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx]
    
    bars = ax.bar(range(len(comparison_df)), comparison_df[metric].values, 
                   color=colors_list[idx], alpha=0.7, edgecolor='black', linewidth=1.5)
    
    # Highlight best
    best_idx = comparison_df[metric].idxmax()
    bars[best_idx].set_color(colors_list[idx])
    bars[best_idx].set_edgecolor('gold')
    bars[best_idx].set_linewidth(3)
    
    ax.set_xticks(range(len(comparison_df)))
    ax.set_xticklabels(comparison_df['Technique'], rotation=45, ha='right', fontsize=10)
    ax.set_ylabel(metric, fontsize=12)
    ax.set_title(f'{metric} by Sampling Technique', fontsize=13, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim([0, 1.05])
    
    # Add value labels on bars
    for i, v in enumerate(comparison_df[metric].values):
        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=9, fontweight='bold')

plt.tight_layout()
plt.show()

print("‚úì Visualization complete")
```

---

## %% [markdown]
# ## 9. Train Final Models with Best Technique

```python
# Use best sampling technique
best_sampler_name = best_technique['Technique']

if best_sampler_name != 'Original (No Sampling)':
    best_sampler = sampling_techniques[best_sampler_name]
    X_train_final, y_train_final = best_sampler.fit_resample(X_train_selected, y_train)
    print(f"‚úì Using sampling: {best_sampler_name}")
else:
    X_train_final, y_train_final = X_train_selected, y_train
    print("‚úì Using original (unbalanced) data")

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, learning_rate=0.1)
}

# Reusable evaluation function
def evaluate_model(y_true, y_pred, y_proba, model_name):
    """Evaluate model and return comprehensive metrics"""
    
    metrics = {
        'Model': model_name,
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, zero_division=0, average='weighted'),
        'Recall': recall_score(y_true, y_pred, zero_division=0, average='weighted'),
        'F1': f1_score(y_true, y_pred, zero_division=0, average='weighted'),
    }
    
    try:
        metrics['ROC_AUC'] = roc_auc_score(y_true, y_proba, multi_class='ovr', zero_division=0)
    except:
        metrics['ROC_AUC'] = 0.0
    
    return metrics

# Train all models
final_results = []
trained_models = {}

print("\n" + "="*60)
print("TRAINING FINAL MODELS")
print("="*60)

for model_name, model in models.items():
    print(f"\nTraining {model_name}...")
    
    # Train
    model.fit(X_train_final, y_train_final)
    trained_models[model_name] = model
    
    # Predict
    y_pred = model.predict(X_test_selected)
    y_proba = model.predict_proba(X_test_selected)
    
    if y_proba.shape[1] == 2:
        y_proba_binary = y_proba[:, 1]
    else:
        y_proba_binary = y_proba.max(axis=1)
    
    # Evaluate
    metrics = evaluate_model(y_test, y_pred, y_proba_binary, model_name)
    final_results.append(metrics)
    
    print(f"  ‚úì Accuracy: {metrics['Accuracy']:.4f}")
    print(f"  ‚úì F1-Score: {metrics['F1']:.4f}")
    print(f"  ‚úì ROC-AUC: {metrics['ROC_AUC']:.4f}")

# Create results DataFrame
final_df = pd.DataFrame(final_results)

print("\n" + "="*60)
print("FINAL MODEL COMPARISON")
print("="*60)
print(final_df.to_string(index=False))

# Find best model
best_model_idx = final_df['F1'].idxmax()
best_model_name = final_df.loc[best_model_idx, 'Model']
best_model = trained_models[best_model_name]

print(f"\nüèÜ BEST MODEL: {best_model_name}")
print(f"   F1-Score: {final_df.loc[best_model_idx, 'F1']:.4f}")
```

---

## %% [markdown]
# ## 10. ROC Curves Comparison

```python
# Plot ROC curves for all models
fig, ax = plt.subplots(figsize=(10, 8))

colors = ['#3498db', '#e74c3c', '#2ecc71']

for idx, (model_name, model) in enumerate(trained_models.items()):
    y_proba = model.predict_proba(X_test_selected)
    
    if y_proba.shape[1] == 2:
        y_proba_binary = y_proba[:, 1]
    else:
        y_proba_binary = y_proba.max(axis=1)
    
    fpr, tpr, _ = roc_curve(y_test, y_proba_binary)
    roc_auc = auc(fpr, tpr)
    
    ax.plot(fpr, tpr, color=colors[idx], lw=2.5, 
            label=f'{model_name} (AUC = {roc_auc:.3f})')

# Random classifier line
ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')

ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])
ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')
ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')
ax.set_title('ROC Curves - Model Comparison', fontsize=15, fontweight='bold')
ax.legend(loc='lower right', fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("‚úì ROC curves plotted")
```

---

## %% [markdown]
# ## 11. Precision-Recall Curves (Top 3 Models)

```python
# Plot Precision-Recall curves
fig, ax = plt.subplots(figsize=(10, 8))

colors = ['#3498db', '#e74c3c', '#2ecc71']

for idx, (model_name, model) in enumerate(trained_models.items()):
    y_proba = model.predict_proba(X_test_selected)
    
    if y_proba.shape[1] == 2:
        y_proba_binary = y_proba[:, 1]
    else:
        y_proba_binary = y_proba.max(axis=1)
    
    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba_binary)
    avg_precision = average_precision_score(y_test, y_proba_binary)
    
    ax.plot(recall_vals, precision_vals, color=colors[idx], lw=2.5,
            label=f'{model_name} (AP = {avg_precision:.3f})')

ax.set_xlabel('Recall', fontsize=13, fontweight='bold')
ax.set_ylabel('Precision', fontsize=13, fontweight='bold')
ax.set_title('Precision-Recall Curves - Model Comparison', fontsize=15, fontweight='bold')
ax.legend(loc='best', fontsize=11)
ax.grid(True, alpha=0.3)
ax.set_xlim([0, 1])
ax.set_ylim([0, 1])

plt.tight_layout()
plt.show()

print("‚úì Precision-Recall curves plotted")
```

---

## %% [markdown]
# ## 12. Best Model Confusion Matrix

```python
# Get predictions from best model
y_pred_best = best_model.predict(X_test_selected)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_best)

# Plot
fig, ax = plt.subplots(figsize=(8, 6))

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'],
            ax=ax, annot_kws={'size': 14})

ax.set_title(f'Confusion Matrix - {best_model_name}', fontsize=15, fontweight='bold')
ax.set_ylabel('True Label', fontsize=13, fontweight='bold')
ax.set_xlabel('Predicted Label', fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

print("‚úì Confusion matrix visualized")
```

---

## %% [markdown]
# ## 13. Classification Report

```python
print("\n" + "="*60)
print(f"CLASSIFICATION REPORT - {best_model_name}")
print("="*60 + "\n")

print(classification_report(y_test, y_pred_best, 
                           target_names=['Not Dropout', 'Dropout'],
                           digits=4))
```

---

## %% [markdown]
# ## 14. Feature Importance Analysis

```python
# Get feature importance
if hasattr(best_model, 'feature_importances_'):
    feature_importance = best_model.feature_importances_
    feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]
    
    # Create DataFrame
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importance
    }).sort_values('Importance', ascending=False).head(15)
    
    # Plot
    fig, ax = plt.subplots(figsize=(10, 8))
    
    bars = ax.barh(range(len(importance_df)), importance_df['Importance'].values,
                    color='#3498db', alpha=0.7, edgecolor='black', linewidth=1.5)
    
    ax.set_yticks(range(len(importance_df)))
    ax.set_yticklabels(importance_df['Feature'].values, fontsize=11)
    ax.set_xlabel('Importance Score', fontsize=13, fontweight='bold')
    ax.set_title(f'Top 15 Important Features - {best_model_name}', fontsize=15, fontweight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)
    
    # Add value labels
    for i, v in enumerate(importance_df['Importance'].values):
        ax.text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.show()
    
    print("‚úì Feature importance plotted")
    print("\nTop 10 Most Important Features:")
    print(importance_df.head(10).to_string(index=False))
else:
    print("Model does not support feature importance")
```

---

## %% [markdown]
# ## 15. Model Export for Production

```python
print("\n" + "="*60)
print("EXPORTING MODEL FOR PRODUCTION")
print("="*60)

# Create models directory
import os
os.makedirs('models', exist_ok=True)

# Save model
model_version = "v1.0_" + datetime.now().strftime("%Y%m%d_%H%M%S")
model_path = f'models/dropout_model_{model_version}.pkl'

joblib.dump(best_model, model_path)
print(f"\n‚úì Model saved: {model_path}")

# Save preprocessor
preprocessor_path = f'models/preprocessor_{model_version}.pkl'
joblib.dump(preprocessor, preprocessor_path)
print(f"‚úì Preprocessor saved: {preprocessor_path}")

# Save feature selector
selector_path = f'models/feature_selector_{model_version}.pkl'
joblib.dump(feature_selector, selector_path)
print(f"‚úì Feature selector saved: {selector_path}")

# Save metadata
metadata = {
    "model_version": model_version,
    "model_type": type(best_model).__name__,
    "training_date": datetime.now().isoformat(),
    "accuracy": float(final_df.loc[best_model_idx, 'Accuracy']),
    "precision": float(final_df.loc[best_model_idx, 'Precision']),
    "recall": float(final_df.loc[best_model_idx, 'Recall']),
    "f1_score": float(final_df.loc[best_model_idx, 'F1']),
    "roc_auc": float(final_df.loc[best_model_idx, 'ROC_AUC']),
    "best_sampling_technique": best_sampler_name,
    "target_classes": ["Not Dropout", "Dropout"],
    "numerical_features": numerical_cols,
    "categorical_features": categorical_cols
}

metadata_path = f'models/metadata_{model_version}.json'
with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=2)

print(f"‚úì Metadata saved: {metadata_path}")

print("\n‚úì All models exported successfully!")
print(f"\nModels are ready for API integration:")
print(f"  - Model: {model_path}")
print(f"  - Preprocessor: {preprocessor_path}")
print(f"  - Feature Selector: {selector_path}")
print(f"  - Metadata: {metadata_path}")
```

---

## %% [markdown]
# ## 16. Summary & Key Insights

```python
print("\n" + "="*60)
print("PROJECT SUMMARY - FINAL RESULTS")
print("="*60)

summary_text = f"""
DATASET:
  - Total students: {len(df)}
  - Training samples: {len(X_train)}
  - Testing samples: {len(X_test)}
  - Class imbalance ratio: {imbalance_ratio:.2f}

DATA PREPROCESSING:
  - Numerical features: {len(numerical_cols)}
  - Categorical features: {len(categorical_cols)}
  - Features after selection: {X_train_selected.shape[1]}

CLASS IMBALANCE HANDLING:
  - Best technique: {best_sampler_name}
  - F1-Score improvement: {(comparison_df['F1'].max() - comparison_df['F1'].iloc[0])*100:.1f}%

MODEL PERFORMANCE:
  - Best model: {best_model_name}
  - Accuracy: {final_df.loc[best_model_idx, 'Accuracy']:.4f}
  - Precision: {final_df.loc[best_model_idx, 'Precision']:.4f}
  - Recall: {final_df.loc[best_model_idx, 'Recall']:.4f}
  - F1-Score: {final_df.loc[best_model_idx, 'F1']:.4f}
  - ROC-AUC: {final_df.loc[best_model_idx, 'ROC_AUC']:.4f}

KEY INSIGHTS FOR MINISTRY:
  1. Student dropout is a {imbalance_ratio:.1f}x more common than expected
  2. {best_model_name} correctly identifies {final_df.loc[best_model_idx, 'Recall']*100:.1f}% of at-risk students
  3. Early intervention needed for identified high-risk students
  4. {importance_df.iloc[0]['Feature']} is the strongest predictor of dropout

NEXT STEPS:
  1. ‚úì Deploy model as REST API
  2. ‚úì Integrate with Airflow for daily predictions
  3. ‚úì Set up monitoring dashboards
  4. ‚úì Create intervention strategies
  5. ‚úì Train ministry staff on system
"""

print(summary_text)

print("="*60)
print("‚úì ANALYSIS COMPLETE - READY FOR DEPLOYMENT")
print("="*60)
```

---

## %% [markdown]
# ## Notes for Team Integration

```python
# CODE STRUCTURE FOR YOUR TEAM:
# ==============================
# Person 1: Data Preprocessing (Sections 1-6)
# Person 2: Model Training (Sections 7-9)  
# Person 3: Model Evaluation & Visualization (Sections 10-14)
# Person 4: API Development (Will use Section 15 exports)
# Person 5: Monitoring & Deployment (Will use metadata & models)

print("\n‚úì Notebook ready for team collaboration!")
print("\nGitHub structure:")
print("""
project-repo/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ student_dropout_analysis.ipynb  (this file)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ dropout_model_v1.0.pkl
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor_v1.0.pkl
‚îÇ   ‚îú‚îÄ‚îÄ feature_selector_v1.0.pkl
‚îÇ   ‚îî‚îÄ‚îÄ metadata_v1.0.json
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py
‚îÇ   ‚îî‚îÄ‚îÄ alerts.py
‚îî‚îÄ‚îÄ README.md
""")
```

---

## **All sections now included:**
‚úÖ Setup & Dependencies  
‚úÖ Data Loading & EDA  
‚úÖ Preprocessing (Once!)  
‚úÖ Train/Test Split (Once!)  
‚úÖ Class Imbalance Analysis  
‚úÖ Oversampling Comparison  
‚úÖ Visualizations of Techniques  
‚úÖ Model Training  
‚úÖ ROC Curves  
‚úÖ Precision-Recall Curves  
‚úÖ Confusion Matrix  
‚úÖ Classification Report  
‚úÖ Feature Importance  
‚úÖ Model Export  
‚úÖ Summary & Insights  

**Ready to use immediately! üöÄ**
